{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive Recurrent Nets with Attention Modeling for OCR in the Wild -- an Implementation Attempt\n",
    "\n",
    " - Spencer Cain and William Goble\n",
    "\n",
    "For our term project we wanted to implement an OCR model and then explore different methods for how the model can be improved. Our model is based off the paper 'Recursive Recurrent Nets with Attention Modeling for OCR in the Wild' by Chen-Yu Lee et al. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is OCR?\n",
    "\n",
    "OCR, or optical character recognition, is the machine learning task of reading text from image data. Versions of this are important in location- and advertisement-based applications in mapping software such as Google Maps or Apple Maps. Further, similar approaches can be used for the recently released text selection from images on iPhones. Another use case of OCR is for the purposes of reading license plates while policing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "We considered the Synth1K, Synth90K, and the IIIT5K Datasets for this project. The Synth1K and Synth90K are datasets that contain generated images of text, the former being of size 1k and the latter being 90k. The IIIT5K Dataset is a 5k word dataset harvested from Google image search. All three of these datasets contained the images and a label, but only the IIIT5K Dataset contained bounding boxes around the characters. We will address why we wanted to use bounding boxes for the characters in our theory section. \n",
    "\n",
    "When cleaning the data we needed to resize all the images to conform with Lee et al.'s input shape, namely 100 by 32. Since we resized the images, this also meant that we needed to scale the bounding box markers of IIIT5K so it would still bound the right characters. After rescaling, we also gray-scaled the images to provide easier computation for the convolutional neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Structure\n",
    "\n",
    "From the paper,\n",
    "\n",
    "> The network architecture for our Base CNN model is\n",
    "shown in Table A1. It has 8 convolutional layer with 64, 64,\n",
    "128, 128, 256, 256, 512 and 512 channels, and each convo-\n",
    "lutional layer uses kernel with a 3 x 3 spatial extent. Convo-\n",
    "lutions are performed with stride 1, zero padding, and ReLU\n",
    "activation function. 2 x 2 max pooling follows the second,\n",
    "fourth, and sixth convolutional layers. The two fully con-\n",
    "nected layers have 4096 units. The input is a resized 32 x\n",
    "100 gray scale image.\n",
    "\n",
    "> We now provide details for the network structures of\n",
    "the proposed untied recursive CNNs in Table A1. Notice\n",
    "that each of the even number convolutional layer (conv2,\n",
    "conv4, conv6 or conv8) use its own shared weight matrix\n",
    "that has exactly the same input and output dimensional-\n",
    "ity, and so projects feature maps to the same space multi-\n",
    "ple times within one recursive convolutional layer under the\n",
    "same parametric capacity as Base CNN model.\n",
    "For the character-level language modeling, we use RNNs\n",
    "with 1024 hidden units equipped with hyperbolic tangent\n",
    "activation function. Our overall system pipeline is shown in\n",
    "Figure 1.\n",
    "\n",
    "> We apply backpropagation through time (BPTT) algo-\n",
    "rithm to train the models with 256 batch size SGD and 0.5\n",
    "dropout rate. Initial learning rate is 0.002 and decreased\n",
    "by a factor of 5 as validation errors stop decreasing for 2\n",
    "epochs. All variants use the same scheme with 30 total\n",
    "epochs determined based on the validation set. We apply\n",
    "gradient clipping at the magnitude of 10, and find it with\n",
    "in place weight decay did not add extra performance gains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import string\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The energy based attention model placed between the two RNN layers.\n",
    "class EnergyAttention(nn.Module):\n",
    "    def __init__(self, I_shape, s_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        self.s_shape = s_shape\n",
    "        self.out_shape = I_shape\n",
    "        \n",
    "        self.s_layer = nn.Linear(self.s_shape, self.out_shape)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        \n",
    "    def forward(self, I, s):\n",
    "        s_proj = self.s_layer(s)\n",
    "        result = self.tanh(I + s_proj)  # -> [1, 1, 1024]\n",
    "        \n",
    "        alpha = self.softmax(result)  # -> [1, 1, 1024]\n",
    "        return alpha * I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The R^2AM Model.\n",
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, eow, recursions=1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.sow_size = eow.size(0)  # SOW = 0, 0, ..., 0, 0\n",
    "        self.eow = eow  # EOW = 0, 0, ..., 0, 1\n",
    "        self.recursions = recursions\n",
    "        \n",
    "        self.conv1_0 = nn.Conv2d(1, 64, (3, 3), padding=\"same\")\n",
    "        self.conv1_t = nn.Conv2d(64, 64, (3, 3), padding=\"same\")\n",
    "\n",
    "        self.pool1 = nn.MaxPool2d((2, 2), stride=2)\n",
    "\n",
    "        self.conv2_0 = nn.Conv2d(64, 128, (3, 3), padding=\"same\")\n",
    "        self.conv2_t = nn.Conv2d(128, 128, (3, 3), padding=\"same\")\n",
    "\n",
    "        self.pool2 = nn.MaxPool2d((2, 2), stride=2)\n",
    "\n",
    "        self.conv3_0 = nn.Conv2d(128, 256, (3, 3), padding=\"same\")\n",
    "        self.conv3_t = nn.Conv2d(256, 256, (3, 3), padding=\"same\")\n",
    "\n",
    "        self.pool3 = nn.MaxPool2d((2, 2), stride=2)\n",
    "\n",
    "        self.conv4_0 = nn.Conv2d(256, 512, (3, 3), padding=\"same\")\n",
    "        self.conv4_t = nn.Conv2d(512, 512, (3, 3), padding=\"same\")\n",
    "\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.dense1 = nn.Linear(24576, 4096)\n",
    "        self.dense2 = nn.Linear(4096, 4096)\n",
    "\n",
    "        self.rnn_1 = nn.LSTM(input_size=self.sow_size, hidden_size=1024,\n",
    "                             num_layers=1, batch_first=True,\n",
    "                             proj_size=self.eow.size(0))\n",
    "\n",
    "        self.attention = EnergyAttention(4096, self.eow.size(0))\n",
    "\n",
    "        self.rnn_2 = nn.LSTM(input_size=4096, hidden_size=1024,\n",
    "                             num_layers=1, batch_first=True,\n",
    "                             proj_size=self.eow.size(0))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1_0(x)\n",
    "        for _ in range(self.recursions):\n",
    "            x = self.conv1_t(x)\n",
    "        \n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = self.conv2_0(x)\n",
    "        for _ in range(self.recursions):\n",
    "            x = self.conv2_t(x)\n",
    "\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = self.conv3_0(x)\n",
    "        for _ in range(self.recursions):\n",
    "            x = self.conv3_t(x)\n",
    "\n",
    "        x = self.pool3(x)\n",
    "\n",
    "        x = self.conv4_0(x)\n",
    "        for _ in range(self.recursions):\n",
    "            x = self.conv4_t(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "\n",
    "        I = torch.unsqueeze(x, dim=1)\n",
    "\n",
    "        batch_size = x.size(0)\n",
    "        batched_sow = torch.autograd.Variable(torch.zeros(size=(x.size(0), 1, self.sow_size))).to(device)\n",
    "        h0 = torch.autograd.Variable(torch.zeros(1, batch_size, self.eow.size(0))).to(device)\n",
    "        c0 = torch.autograd.Variable(torch.zeros(1, batch_size, 1024)).to(device)\n",
    "        results = torch.autograd.Variable(torch.zeros(batch_size, self.sow_size, 23)).to(device)\n",
    "\n",
    "        s, (hn_1, cn_1) = self.rnn_1(batched_sow, (h0, c0))\n",
    "        c_t = self.attention(I, s)\n",
    "        x, (hn_2, cn_2) = self.rnn_2(c_t, (h0, c0))\n",
    "        results[:, :, 0] = torch.squeeze(x, dim=1)\n",
    "        for idx in range(1, 23):\n",
    "            s, (hn_1, cn_1) = self.rnn_1(x, (hn_1, cn_1))\n",
    "            c_t = self.attention(I, s)\n",
    "            x, (hn_2, cn_2) = self.rnn_2(c_t, (hn_2, cn_2))\n",
    "            results[:, :, idx] = torch.squeeze(x, dim=1)\n",
    "\n",
    "        return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\"
     ]
    }
   ],
   "source": [
    "eow = torch.zeros(size=(len(string.printable) + 1,))\n",
    "eow[len(string.printable)] = 1\n",
    "\n",
    "base_cnn_model = BaseModel(eow=eow).to(device)\n",
    "\n",
    "x = torch.rand(1, 1, 32, 100).to(device)\n",
    "x = base_cnn_model(x)\n",
    "\n",
    "preds = torch.argmax(x[0].T, dim=1).tolist()  # [0] because its the first item in a batch size of 1\n",
    "for pred in preds:\n",
    "    print(string.printable[pred], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IIIT5K.dataset import IIIT5KDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_set = IIIT5KDataset(split='train')\n",
    "val_set = IIIT5KDataset(split='val')\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model_name: str, num_epochs: int):\n",
    "    model = BaseModel(eow=eow).to(device)\n",
    "    print('Total Parameters:', sum(p.numel() for p in model.parameters()))\n",
    "    print('Trainable Parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=.002)\n",
    "    running_loss = 0.\n",
    "    for epoch in range(num_epochs):\n",
    "        for step, (image, label) in enumerate(train_loader):\n",
    "            image = torch.unsqueeze(image, dim=1).to(device)\n",
    "            label = torch.stack(label, dim=0).to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(image)\n",
    "            loss = criterion(output, label.T)\n",
    "            loss.backward()\n",
    "            \n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Gather data and report\n",
    "            running_loss += loss.item()\n",
    "        last_loss = running_loss / step # loss per batch\n",
    "        \n",
    "        acc, total = 0., 0.\n",
    "        for o, l in zip(output, label):\n",
    "            o = torch.argmax(o, dim=1)\n",
    "            for o_, l_ in zip(o, l):\n",
    "                if l_ == 100:\n",
    "                    break\n",
    "                if o_ == l_:\n",
    "                    acc += 1.\n",
    "                total += 1.\n",
    "                \n",
    "        acc = acc / total\n",
    "        print(f'epoch {epoch+1} -> train_loss: {last_loss:.4f}, train_acc: {acc:.4f}')\n",
    "\n",
    "        running_loss = 0.\n",
    "\n",
    "        # Validation\n",
    "        val_loss = 0.\n",
    "        val_acc = 0.\n",
    "        val_total = 0.\n",
    "        with torch.no_grad():\n",
    "            for step, (image, label) in enumerate(val_loader):\n",
    "                image = torch.unsqueeze(image, dim=1).to(device)\n",
    "                label = torch.stack(label, dim=0).to(device)\n",
    "\n",
    "                output = model(image)\n",
    "                loss = criterion(output, label.T)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                for o, l in zip(output, label):\n",
    "                    o = torch.argmax(o, dim=1)\n",
    "                    for o_, l_ in zip(o, l):\n",
    "                        if l_ == 100:\n",
    "                            break\n",
    "                        if o_ == l_:\n",
    "                            val_acc += 1.\n",
    "                        val_total += 1.\n",
    "            val_acc = val_acc / val_total\n",
    "            val_loss = val_loss / step\n",
    "            print(f'epoch {epoch+1} -> val_loss: {val_loss:.4f}, val_acc: {val_acc:.4f}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 140792256\n",
      "Trainable Parameters: 140792256\n",
      "epoch 1 -> train_loss: 4.9184, train_acc: 0.0000\n",
      "epoch 1 -> val_loss: 9.2156, val_acc: 0.0060\n",
      "epoch 2 -> train_loss: 4.9117, train_acc: 0.0000\n",
      "epoch 2 -> val_loss: 9.2031, val_acc: 0.0109\n",
      "epoch 3 -> train_loss: 4.9049, train_acc: 0.0336\n",
      "epoch 3 -> val_loss: 9.1906, val_acc: 0.0030\n",
      "epoch 4 -> train_loss: 4.8982, train_acc: 0.0000\n",
      "epoch 4 -> val_loss: 9.1780, val_acc: 0.0131\n",
      "epoch 5 -> train_loss: 4.8914, train_acc: 0.0085\n",
      "epoch 5 -> val_loss: 9.1654, val_acc: 0.0200\n",
      "epoch 6 -> train_loss: 4.8845, train_acc: 0.0131\n",
      "epoch 6 -> val_loss: 9.1527, val_acc: 0.0070\n",
      "epoch 7 -> train_loss: 4.8776, train_acc: 0.0088\n",
      "epoch 7 -> val_loss: 9.1398, val_acc: 0.0000\n",
      "epoch 8 -> train_loss: 4.8707, train_acc: 0.0128\n",
      "epoch 8 -> val_loss: 9.1269, val_acc: 0.0026\n",
      "epoch 9 -> train_loss: 4.8637, train_acc: 0.0000\n",
      "epoch 9 -> val_loss: 9.1139, val_acc: 0.0146\n",
      "epoch 10 -> train_loss: 4.8566, train_acc: 0.0064\n",
      "epoch 10 -> val_loss: 9.1006, val_acc: 0.0039\n",
      "epoch 11 -> train_loss: 4.8494, train_acc: 0.0000\n",
      "epoch 11 -> val_loss: 9.0871, val_acc: 0.0063\n",
      "epoch 12 -> train_loss: 4.8421, train_acc: 0.0159\n",
      "epoch 12 -> val_loss: 9.0736, val_acc: 0.0032\n",
      "epoch 13 -> train_loss: 4.8347, train_acc: 0.0088\n",
      "epoch 13 -> val_loss: 9.0597, val_acc: 0.0066\n",
      "epoch 14 -> train_loss: 4.8272, train_acc: 0.0000\n",
      "epoch 14 -> val_loss: 9.0457, val_acc: 0.0117\n",
      "epoch 15 -> train_loss: 4.8196, train_acc: 0.0000\n",
      "epoch 15 -> val_loss: 9.0314, val_acc: 0.0075\n",
      "epoch 16 -> train_loss: 4.8117, train_acc: 0.0214\n",
      "epoch 16 -> val_loss: 9.0168, val_acc: 0.0089\n",
      "epoch 17 -> train_loss: 4.8038, train_acc: 0.0294\n",
      "epoch 17 -> val_loss: 9.0019, val_acc: 0.0031\n",
      "epoch 18 -> train_loss: 4.7957, train_acc: 0.0096\n",
      "epoch 18 -> val_loss: 8.9867, val_acc: 0.0078\n",
      "epoch 19 -> train_loss: 4.7873, train_acc: 0.0000\n",
      "epoch 19 -> val_loss: 8.9710, val_acc: 0.0067\n",
      "epoch 20 -> train_loss: 4.7788, train_acc: 0.0161\n",
      "epoch 20 -> val_loss: 8.9551, val_acc: 0.0031\n",
      "epoch 21 -> train_loss: 4.7702, train_acc: 0.0099\n",
      "epoch 21 -> val_loss: 8.9388, val_acc: 0.0000\n",
      "epoch 22 -> train_loss: 4.7613, train_acc: 0.0057\n",
      "epoch 22 -> val_loss: 8.9220, val_acc: 0.0059\n",
      "epoch 23 -> train_loss: 4.7519, train_acc: 0.0117\n",
      "epoch 23 -> val_loss: 8.9048, val_acc: 0.0000\n",
      "epoch 24 -> train_loss: 4.7426, train_acc: 0.0198\n",
      "epoch 24 -> val_loss: 8.8870, val_acc: 0.0134\n",
      "epoch 25 -> train_loss: 4.7328, train_acc: 0.0000\n",
      "epoch 25 -> val_loss: 8.8688, val_acc: 0.0000\n",
      "epoch 26 -> train_loss: 4.7229, train_acc: 0.0000\n",
      "epoch 26 -> val_loss: 8.8499, val_acc: 0.0134\n",
      "epoch 27 -> train_loss: 4.7125, train_acc: 0.0097\n",
      "epoch 27 -> val_loss: 8.8306, val_acc: 0.0039\n",
      "epoch 28 -> train_loss: 4.7019, train_acc: 0.0078\n",
      "epoch 28 -> val_loss: 8.8104, val_acc: 0.0033\n",
      "epoch 29 -> train_loss: 4.6909, train_acc: 0.0133\n",
      "epoch 29 -> val_loss: 8.7898, val_acc: 0.0039\n",
      "epoch 30 -> train_loss: 4.6796, train_acc: 0.0097\n",
      "epoch 30 -> val_loss: 8.7682, val_acc: 0.0067\n"
     ]
    }
   ],
   "source": [
    "model = train_model(model_name=\"R2AM\", num_epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, the model structure from the paper and dataset used to generate results in the paper did not work in our implementation. While implementing the model, we ran into some issues with the `torch` API, including the optimizer not updating the weights and the loss not changing throughout training. We explored several different versions of the model to troubleshoot the issues, which reduced our time to implement our novel theories. Unfortunately, we could not get past these issues. Missing from the paper's architecture in comparison to ours is a scheduled learning rate, dropout, and weight decay. However, these missing components do not seem to be the likely cause behind our results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Novel Theory\n",
    "\n",
    "Our confusion when reading the paper by Lee et al. involved how the RNN layers could read letters from the linear layers. Since linear layers are not sequential by nature, we believed that this could be one of the larger difficulties that the model needed to learn from. Our biggest concern was how the RNN could read/predict letters from left to right. Could this solely be an implicitly learned task? To provide a novel improvement to the model, we intended to bridge the gap from non-sequential data to sequential data with bounding boxes. This would require two submodels, rather than an end-to-end implementation. Our theory was to train a CNN-based model to predict the locations of letters in an image with bounding boxes. Then, with a pretrained CNN as the bounding box predictor, we would use the bounding boxes in order from left to right to classify letters and use RNN sequence prediction abilities to smooth out any poor classification predictions made by the linear layers. Had we been able to implement this approach, we believe it would perform more consistantly in a wider variety of application cases than the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethical Consideration\n",
    "\n",
    "Optical Character Recognition models (OCR) is one method for converting images of either handwritten, typed, or printed text into machine encoded text. This technology is a useful tool for automating tasks that traditionally require a human to examine an image and determine what text say. One such example is the automation of the traffic violation pipeline. Currently if someone were to run through a red light with a traffic camera, or drive through a toll plaza, a photo of their license plate would be taken, processed, and a bill would be sent to the drivers address. A task such as identifying a license plate, and extracting the license plate number, use to require human intervention, but thanks to technology like OCR models, this task can now be fully automated. Another example of a task OCR models could accomplish is reading signs or products that an individual might have in the background during a virtual meeting, such as over Zoom or Microsoft Teams, and then send targeted ads to the individual to encourage them to buy more, or similar, products. As a quick note, we are not saying any one company is doing this, but it is reasonable to assume this could be done. In both the targeted ads  and the license plate reader scenario, as machine learning engineers we are forced to face the question where does data collection end and personal privacy begin?\n",
    "\n",
    "One important thing to keep in mind when working with OCR models is having transparency with why the model processes a certain type of image text. A lack of transparency regarding OCR model usage is the difference between an automated toll booth system, and a device a police state could use to track the locations of citizens. Like in most fields, transparency is the key to gaining the trust of the users, and encourages the users to buy into the system. When companies are transparent with their intentions of the model, it places the choice back into the hands of the individual, and with their consent can lead to better improvements to the overall system. Additionally, machine learning engineers and companies need to be on gaurd against people trying to abuse their models in order to exploit the users. If the company is transparent about how to protects the users data, this transparency will also help build public trust."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
